<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Fairness in Visual Learning</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet"
        integrity="sha384-9ndCyUaIbzAi2FUVXJi0CjmCapSmO7SnpJef0486qhLnuZ2cdeRhO02iuK6FUUVM" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"
        integrity="sha384-geWF76RCwLtnZ8qwWowPQNguL3RmwHVBC9FhGdlKrxdiJJigb/j/68SIy3Te4Bkz"
        crossorigin="anonymous"></script>
    <link rel="stylesheet" href="style.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="icon" type="image/x-icon" href="/images/icon/favicon.ico">

    <!--script defer src="script.js"></script-->

    <script defer src="tables/celeba_13_attribs.js"></script>
    <script defer src="tables/celeba_eyebags.js"></script>
    <script defer src="tables/celeba_chubby.js"></script>
    <script defer src="tables/covid_cxr.js"></script>
    <script defer src="tables/fitzpatrick.js"></script>
    <script defer src="tables/utkface_skintone.js"></script>
    <script defer src="tables/utkface_age.js"></script>
</head>

<body>
    <div class="container p-5 mb-5 text-center d-flex flex-column align-items-center">

        <h1><b>Advancing Fairness in Visual Learning</b></h1>
        <hr style="border-bottom: 1px black; width: 100%;"/>

        <img src="images/banner.png" class="img-fluid pt-5 pb-3" alt="teaser">

        <!--div id="head-carousel" class="carousel carousel-dark slide pt-5" data-bs-ride="carousel">
            <div id="inner-carousel" class="carousel-inner">
            </div>
        </div-->

        <!--h5 class="fw-bold pt-5">Welcome!</h5-->
        <p class="pt-5" style="text-align: justify;">
            In a world increasingly shaped by artificial intelligence, it is essential to ensure that these technologies 
            are not only accurate and powerful but also equitable and unbiased. In particular, computer vision applications 
            are supportive tools in our daily lives, helping us to store and organize photos, navigate the streets of a new city, 
            choose what to wear or what couch will fit best in our living room. However, they also play a relevant role in 
            more critical decisions including healthcare, policing, employment and more. One aspect that is currently 
            keeping low the level of trust associated with vision systems is their potential unfairness, meaning that the 
            models may exploit spurious sensitive attribute correlations (e.g. age, gender, race) when solving seemingly 
            unrelated tasks on data coming from different demographic groups.
        </p>
        <p class="pt-3" style="text-align: justify;">
            With this project page we would like to contribute to the numerous efforts that the research community is 
            currently pursuing towards fair computer vision models. Specifically we present our work dedicated to the 
            definition of a fairness benchmark which spans both face and medical images for classification and landmark 
            detection tasks.
        </p>

        <h5 class="fw-bold pt-5">Tasks</h5>
        <p class="pt-3" style="text-align: justify;">
            We remark that current unfairness mitigation strategies in computer vision are restricted to classification 
            problems. To overcome this limitation, we include in our benchmark the task of landmark detection on face 
            images of different demographic groups, as the bias related to sensitive attributes can affect the precision 
            with which critical keypoints are located.
        </p>

        <h5 class="fw-bold pt-5">Methods</h5>
        <p class="pt-3" style="text-align: justify;">
            Resolving the fairness issue while maintaining accurate predictions for the target task is a challenge shared 
            with the domain adaptation and generalization literature which focuses on avoiding visual domain biases. 
            Thus, we start our study by investigating the connection between cross-domain learning (CD) and model 
            fairness by evaluating 14 CD learning approaches alongside three state-of-the-art (SOTA) fairness algorithms. 
            We conclude that the former can outperform the latter.
        </p>
        <p class="pt-3 text-start" style="text-align: justify;">
            Here is the list of the evaluated methods...
        </p>
        

        <h5 class="fw-bold pt-5">Metrics</h5>
        <p class="pt-3" style="text-align: justify;">
            Another aspect on which there is still a lot of confusion and open debate is about how systems should be 
            evaluated. There are multiple competing notions of fairness and ways to quantify it. Previous studies 
            measure group fairness by accuracy difference between advantaged and disadvantaged subgroups. However, 
            this goal has been criticized in philosophy and ethics literature. Purely minimizing the gap between 
            subgroup performance, may lead to choosing a model with worse accuracy for all subgroups, which is 
            Pareto inefficient and violates the ethical principles of beneficence and non-maleficence. Thus, we 
            analyze several existing group fairness criteria and highlight the lack of a metric that properly 
            aggregates overall performance and fairness level to assess the quality of a model.
        </p>
        <p class="pt-3 text-start" style="text-align: justify;">
            List of metrics...
        </p>

        <h5 class="fw-bold pt-5">Datasets</h5>
        <p class="pt-3" style="text-align: justify;">
            ...
        </p>

        <h5 class="fw-bold pt-5">Results</h5>
        <p class="pt-3" style="text-align: justify;">
            ...
        </p>
        
        <div class="container pt-5">
            <p class="fw-bold">CelebA - 13 Attributes <i>(gender)</i></p>
            <div class="table-responsive d-flex justify-content-center" id="celeba-13-attribs"></div>

            <p class="fw-bold pt-5">CelebA - EyeBags <i>(gender)</i></p>
            <div class="table-responsive d-flex justify-content-center" id="celeba-eyebags"></div>

            <p class="fw-bold pt-5">CelebA - Chubby <i>(gender)</i></p>
            <div class="table-responsive d-flex justify-content-center" id="celeba-chubby"></div>

            <p class="fw-bold pt-5">COVID-19 Chest X-Ray <i>(gender)</i></p>
            <div class="table-responsive d-flex justify-content-center" id="covid-cxr"></div>

            <p class="fw-bold pt-5">Fitzpatrick17k <i>(skin tone)</i></p>
            <div class="table-responsive d-flex justify-content-center" id="fitzpatrick"></div>

            <p class="fw-bold pt-5">UTKFace - Landmark Detection @ 8% NME</p>
            <p class="fw-bold pt-1"><i>(skin tone)</i></p>
            <div class="table-responsive d-flex justify-content-center" id="utkface-skintone"></div>
            <p class="fw-bold pt-1"><i>(age)</i></p>
            <div class="table-responsive d-flex justify-content-center" id="utkface-age"></div>

            <p class="fw-bold pt-5">CelebA - EyeBags [Model Transferability]</p>
            <div class="table-responsive d-flex justify-content-center" id="celeba-eyebags-transfer"></div>

            <p class="fw-bold pt-5">UTKFace - Landmark Detection [Model Transferability]</p>
            <div class="table-responsive d-flex justify-content-center" id="utkface-transfer"></div>
        </div>
        
        <hr style="border-bottom: 1px black; width: 100%;"/>
        
        <!--h3 class="pt-5"><b>Fairness meets Cross-Domain Learning: a new perspective on Models and Metrics</b></h3>

        <p class="pt-3"><b>Leonardo Iurada<sup>1</sup>, Silvia Bucci<sup>1</sup>, Timothy M. Hospedales<sup>3</sup>,
                Tatiana Tommasi<sup>1,2</sup></b></p>
        <div class="container d-flex justify-content-evenly">
            <p><sup>1</sup>Politecnico di Torino</p>
            <p><sup>2</sup>Italian Institute of Technology</p>
            <p><sup>3</sup>University of Edinburgh</p>
        </div>

        <div class="container d-flex justify-content-center pt-3 pb-3">
            <a href="https://arxiv.org/pdf/2303.14411.pdf" target="_blank"><button type="button" class="btn btn-dark btn-small m-2"><i
                        class="fa fa-newspaper-o"></i> Paper</button></a>
            <a href="https://arxiv.org/abs/2303.14411.pdf" target="_blank"><button type="button" class="btn btn-dark btn-small m-2"><i
                        class="ai ai-arxiv"></i> ArXiv</button></a>
            <a href="https://github.com/iurada/fairness_crossdomain" target="_blank"><button type="button"
                    class="btn btn-dark btn-small m-2"><i class="fa fa-github"></i> Code</button></a>
        </div>

        <img src="images/teaser.png" class="img-fluid pt-5 pb-3 img-teaser" alt="teaser">


        <p class="text-start" style="text-align: justify;"><b>Overview:</b> Starting from a model that exhibits some
            degree of unfairness
            as evidenced by the Difference in Accuracy between protected groups (left). We exploit Cross-Domain
            (CD) learning to reduce the visual domain shift among groups and improve the generalization
            ability of the model, obtaining an unfairness mitigation effect (right).</p>

            <h5 class="fw-bold pt-5">Abstract</h5>
            <p class="fst-italic p-abs" style="text-align: justify;">
                Deep learning-based recognition systems are deployed at scale for several real-world applications
                that inevitably involve our social life. Although being of great support when making complex decisions,
                they might capture spurious data correlations and leverage sensitive attributes (e.g. age, gender,
                ethnicity). How to factor out this information while keeping a high prediction performance is a task
                with still several open questions, many of which are shared with those of the domain adaptation and
                generalization literature which focuses on avoiding visual domain biases. In this work, we propose an
                in-depth study of the relationship between cross-domain learning (CD) and model fairness by introducing a
                benchmark on face and medical images spanning several demographic groups as well as classification and
                localization tasks. After having highlighted the limits of the current evaluation metrics, we introduce
                a new Harmonic Fairness (HF) score to assess jointly how fair and accurate every model is with respect
                to a reference baseline. Our study covers 14 CD approaches alongside three state-of-the-art fairness
                algorithms and shows how the former can outperform the latter. Overall, our work paves the way for a
                more systematic analysis of fairness problems in computer vision.
            </p>

        <div class="container pt-5">
            <h5 class="fw-bold pb-3">Benchmark Results</h5>

            <p class="fw-bold">CelebA - 13 Attributes Experiment</p>
            <div class="table-responsive d-flex justify-content-center" id="celeba-13-attribs"></div>
        </div-->

    </div>
</body>

</html>