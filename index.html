<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Fairness in Visual Learning</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet"
        integrity="sha384-9ndCyUaIbzAi2FUVXJi0CjmCapSmO7SnpJef0486qhLnuZ2cdeRhO02iuK6FUUVM" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"
        integrity="sha384-geWF76RCwLtnZ8qwWowPQNguL3RmwHVBC9FhGdlKrxdiJJigb/j/68SIy3Te4Bkz"
        crossorigin="anonymous"></script>

    <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script defer id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="style.css">
    
    <!-- Favicon -->
    <link rel="apple-touch-icon" sizes="180x180" href="images/icon/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="images/icon/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="images/icon/favicon-16x16.png">
    <link rel="manifest" href="images/icon/site.webmanifest">
    <link rel="mask-icon" href="images/icon/safari-pinned-tab.svg" color="#5bbad5">
    <link rel="shortcut icon" href="images/icon/favicon.ico">
    <meta name="msapplication-TileColor" content="#da532c">
    <meta name="msapplication-config" content="images/icon/browserconfig.xml">
    <meta name="theme-color" content="#ffffff">

    <script defer src="script.js"></script>

    <script defer src="tables/celeba_13_attribs.js"></script>
    <script defer src="tables/celeba_eyebags.js"></script>
    <script defer src="tables/celeba_chubby.js"></script>
    <script defer src="tables/covid_cxr.js"></script>
    <script defer src="tables/fitzpatrick.js"></script>
    <script defer src="tables/utkface_skintone.js"></script>
    <script defer src="tables/utkface_age.js"></script>
    <script defer src="tables/celeba_eyebags_transfer.js"></script>
    <script defer src="tables/utkface_transfer.js"></script>

    <script defer src="references.js"></script>
</head>

<body>
    <div class="container p-5 mb-5 text-center d-flex flex-column align-items-center">
        
        <div class="container fixed-top d-flex p-3" style="background-color: white;">
            <div class="d-flex align-items-center" style="width: 85%;">
                <img src="images/icon/logo.svg" width="45px" alt="..." style="transform: rotate(180deg);">
                <h1 style="padding-left: 5px; position: relative; top: 5px;"><b>Advancing Fairness in Visual Learning</b></h1>
            </div>

            <div>
                <h3 style="text-align: right;"><b>Leonardo Iurada</b></h3>
                <h6 style="text-align: right;"><b>POLITECNICO DI TORINO</b></h6>
                <h6 style="text-align: right; font-family: 'Courier New', Courier, monospace;">leonardo.iurada@polito.it</h6>
            </div>
        </div>
        <div class="mt-5" style="height: 10px;"></div>
        <hr class="mt-5" style="border-bottom: 1px black; width: 100%;"/>

        <img src="images/banner.png" class="img-fluid pt-3 pb-3" alt="teaser">

        <div class="container">
            <h1 class="pt-5" style="text-align: center;">
                <b>Advancing Fairness with Cross-Domain Learning</b>
            </h1>
            <div class="container d-flex align-items-center p-1 justify-content-evenly" style="width: 60%;">
                <h6 style="width: 100%;"><b>L. Iurada<sup>1</sup></b></h6>
                <h6 style="width: 100%;"><b>S. Bucci<sup>1</sup></b></h6>
                <h6 style="width: 100%;"><b>T. M. Hospedales<sup>3</sup></b></h6>
                <h6 style="width: 100%;"><b>T. Tommasi<sup>1,2</sup></b></h6>
            </div>
            <div class="container d-flex align-items-center justify-content-evenly" style="width: 85%;">
                <h6 style="width: 100%;"><b><sup>1</sup>POLITECNICO DI TORINO</b></h6>
                <h6 style="width: 100%;"><b><sup>2</sup>ITALIAN INSTITUTE OF TECHNOLOGY</b></h6>
                <h6 style="width: 100%;"><b><sup>3</sup>UNIVERSITY OF EDINBURGH</b></h6>
            </div>
            <div class="container d-flex align-items-center justify-content-evenly" style="width: 85%;">
                <h6 style="width: 100%; font-family: 'Courier New', Courier, monospace">{leonardo.iurada, silvia.bucci, tatiana.tommasi}@polito.it, t.hospedales@ed.ac.uk</h6>
            </div>
            <div class="container d-flex align-items-center justify-content-evenly pt-3" style="width: 85%;">
                <a href="https://sites.google.com/view/wicviccv2023/home" target="_blank"><button type="button" class="btn btn-dark btn-small m-2"><i
                    class="fa fa-newspaper-o"></i> Paper</button></a>
                <a href="https://arxiv.org/abs/2303.14411" target="_blank"><button type="button" class="btn btn-dark btn-small m-2"><i
                            class="ai ai-arxiv"></i> ArXiv</button></a>
                <a href="https://github.com/iurada/fairness_crossdomain" target="_blank"><button type="button"
                        class="btn btn-dark btn-small m-2"><i class="fa fa-github"></i> Code</button></a>
            </div>
        </div>

        <p class="pt-5" style="text-align: left;">
            In a world increasingly shaped by artificial intelligence, it is essential to ensure that these technologies 
            are not only accurate and powerful but also equitable and unbiased. In particular, computer vision applications 
            are supportive tools in our daily lives, helping us to store and organize photos, navigate the streets of a new city, 
            choose what to wear or what couch will fit best in our living room. However, they also play a relevant role in 
            more critical decisions including healthcare, policing, employment and more. One aspect that is currently 
            keeping low the level of trust associated with vision systems is their potential unfairness, meaning that the 
            models may exploit spurious sensitive attribute correlations (e.g. age, gender, race) when solving seemingly 
            unrelated tasks on data coming from different demographic groups.
        </p>
        <p class="pt-3" style="text-align: left;">
            With this project page we would like to contribute to the numerous efforts that the research community is 
            currently pursuing towards fair computer vision models. Specifically we present our work dedicated to the 
            definition of a fairness benchmark which spans both face and medical images for classification and landmark 
            detection tasks.
        </p>

        <p class="fw-bold pt-5" style="text-align: left; font-size: 2rem;">Tasks</p>
        <p style="text-align: left;">
            We remark that current unfairness mitigation strategies in computer vision are restricted to classification 
            problems. To overcome this limitation, we include in our benchmark the task of landmark detection on face 
            images of different demographic groups, as the bias related to sensitive attributes can affect the precision 
            with which critical keypoints are located.
        </p>

        <p class="fw-bold pt-5" style="text-align: left; font-size: 2rem;">Methods</p>
        <p style="text-align: left;">
            Resolving the fairness issue while maintaining accurate predictions for the target task is a challenge shared 
            with the domain adaptation and generalization literature which focuses on avoiding visual domain biases. 
            Thus, we start our study by investigating the connection between cross-domain learning (CD) and model 
            fairness by evaluating 14 CD learning approaches alongside three state-of-the-art (SOTA) fairness algorithms. 
            We conclude that the former can outperform the latter.
        </p>
        <p class="pt-3 text-start d-flex" style="text-align: left;">Here is the list of the evaluated methods:</p>
        <div class="table-responsive p-abs" style="border: 0;">
            <table class="table table-sm table-borderless">
                <tbody>
                    <tr>
                        <th style="text-align: right; padding-right: 20px;">Baseline</th>
                        <td style="text-align: left;">
                            <p>
                                For our classification experiments we follow the fairness literature [<a id="a1" onclick="scrl('r1')" class="hp">1</a>] adopting as baseline ResNet50
                                with standard cross-entropy minimization objective, pretrained on ImageNet.
                            </p>
                            <p>
                                For landmark detection we follow [<a id="a2" onclick="scrl('r2')" class="hp">2</a>, <a id="a3" onclick="scrl('r3')" class="hp">3</a>] 
                                and consider ResNet18 pre-trained on ImageNet with a dedicated 
                                head composed of deconvolutional layers. The network is optimized with an 
                                L2 loss to reduce the discrepancy between the predicted probability distribution 
                                of the location of each landmark and the ground truth.
                            </p>
                        </td>
                    </tr>

                    <tr>
                        <th style="text-align: right; border-bottom: 1px solid black;" colspan="2">Classification</th>
                    </tr>
                    <tr><th style="text-align: left;" colspan="1">Regularization-Based Methods</th></tr>
                    <tr><td style="text-align: left;" colspan="1">
                        Include all the techniques designed to prevent overfitting with a consequent boost in the model 
                        generalization ability.
                    </td></tr>
                    <tr>
                        <th style="text-align: right; padding-right: 20px;">LSR</th>
                        <td style="text-align: left;">
                            Encourages the model to avoid overconfidence by smoothing data annotation. [<a id="a4" onclick="scrl('r4')" class="hp">4</a>]
                        </td>
                    </tr>
                    <tr>
                        <th style="text-align: right; padding-right: 20px;">SWAD</th>
                        <td style="text-align: left;">
                            Searches for flat minima. [<a id="a5" onclick="scrl('r5')" class="hp">5</a>]
                        </td>
                    </tr>
                    <tr>
                        <th style="text-align: right; padding-right: 20px;">RSC</th>
                        <td style="text-align: left;">
                            Is based on a refined drop-out. [<a id="a6" onclick="scrl('r6')" class="hp">6</a>]
                        </td>
                    </tr>
                    <tr>
                        <th style="text-align: right; padding-right: 20px;">L2D</th>
                        <td style="text-align: left;">
                            Includes a module trained to synthesize new images with a style distribution 
                            complementary to that of the training data. [<a id="a7" onclick="scrl('r7')" class="hp">7</a>]
                        </td>
                    </tr>
                    
                    <tr><th style="text-align: left;" colspan="1">Adversarial Training Methods</th></tr>
                    <tr><td style="text-align: left;" colspan="1">
                        Encode domain-invariant representations by preventing the network to recognize the domains.
                    </td></tr>
                    <tr>
                        <th style="text-align: right; padding-right: 20px;">DANN</th>
                        <td style="text-align: left;">
                            The gradient computed by a domain discriminator is inverted while learning the 
                            data representation. [<a id="a8" onclick="scrl('r8')" class="hp">8</a>]
                        </td>
                    </tr>
                    <tr>
                        <th style="text-align: right; padding-right: 20px;">CDANN</th>
                        <td style="text-align: left;">
                            Improves over <b>DANN</b> by matching the conditional data distributions
                            across domains rather than the marginal distributions. [<a id="a9" onclick="scrl('r9')" class="hp">9</a>]
                        </td>
                    </tr>
                    <tr>
                        <th style="text-align: right; padding-right: 20px;">SagNets</th>
                        <td style="text-align: left;">
                            Introduces dedicated data randomizations to disentangle style from 
                            class encodings. [<a id="a10" onclick="scrl('r10')" class="hp">10</a>]
                        </td>
                    </tr>

                    <tr><th style="text-align: left;" colspan="1">Feature Alignment Methods</th></tr>
                    <tr><td style="text-align: left;" colspan="1">
                        Involve training objectives that minimize domain distance measures.
                    </td></tr>
                    <tr>
                        <th style="text-align: right; padding-right: 20px;">AFN</th>
                        <td style="text-align: left;">
                            Measures domain shift by comparing the feature norms of two domains and adapts 
                            them to a common large value. [<a id="a11" onclick="scrl('r11')" class="hp">11</a>]
                        </td>
                    </tr>
                    <tr>
                        <th style="text-align: right; padding-right: 20px;">MMD</th>
                        <td style="text-align: left;">
                            Minimizes the homonym metric to reduce the domain discrepancy. [<a id="a12" onclick="scrl('r12')" class="hp">12</a>]
                        </td>
                    </tr>
                    <tr>
                        <th style="text-align: right; padding-right: 20px;">Fish</th>
                        <td style="text-align: left;">
                            Proposes to align the domain distributions by maximizing the inner 
                            product between their gradients. [<a id="a13" onclick="scrl('r13')" class="hp">13</a>]
                        </td>
                    </tr>

                    <tr><th style="text-align: left;" colspan="1">Self-Supervised Learning-based Methods</th></tr>
                    <tr><td style="text-align: left;" colspan="1">
                        Exploit auxiliary self-supervised tasks to let the network focus on semantic-relevant features.
                    </td></tr>
                    <tr>
                        <th style="text-align: right; padding-right: 20px;">RelRot</th>
                        <td style="text-align: left;">
                            Predicts the relative orientation between a reference image (anchor) and the 
                            rotated counterpart as auxiliary task. [<a id="a14" onclick="scrl('r14')" class="hp">14</a>]
                        </td>
                    </tr>
                    <tr>
                        <th style="text-align: right; padding-right: 20px;">RelRotAlign</th>
                        <td style="text-align: left;">
                            A variant of <b>RelRot</b> to encourage the domain alignment using as anchor a 
                            sample with the same target attribute but from a different 
                            protected group.
                        </td>
                    </tr>
                    <tr>
                        <th style="text-align: right; padding-right: 20px;">SelfReg</th>
                        <td style="text-align: left;">
                            Exploits contrastive losses to regularize the model and guide it to learn 
                            domain-invariant representations. [<a id="a15" onclick="scrl('r15')" class="hp">15</a>]
                        </td>
                    </tr>

                    <tr><th style="text-align: left;" colspan="1">State-of-the-art Fairness Methods</th></tr>
                    <tr><td style="text-align: left;" colspan="1">
                        These approaches are tailored specifically to mitigate unfairness.
                    </td></tr>
                    <tr>
                        <th style="text-align: right; padding-right: 20px;">GroupDRO</th>
                        <td style="text-align: left;">
                            Minimizes the worst-case training loss over a set of pre-defined 
                            groups. [<a id="a16" onclick="scrl('r16')" class="hp">16</a>]
                        </td>
                    </tr>
                    <tr>
                        <th style="text-align: right; padding-right: 20px;">g-SMOTE</th>
                        <td style="text-align: left;">
                            Is a generative approach that reduces unfairness by synthesizing 
                            new samples of the most disadvantaged 
                            group. [<a id="a17" onclick="scrl('r17')" class="hp">17</a>]
                        </td>
                    </tr>
                    <tr>
                        <th style="text-align: right; padding-right: 20px;">FSCL</th>
                        <td style="text-align: left;">
                            Re-designs supervised contrastive learning to ensure fairness by paying 
                            attention to the choice of the negative samples and to the distribution 
                            of the anchors between data groups. [<a id="a18" onclick="scrl('r18')" class="hp">18</a>]
                        </td>
                    </tr>

                    <tr>
                        <th style="text-align: right; border-bottom: 1px solid black;" colspan="2">Landmark Detection</th>
                    </tr>
                    <tr>
                        <th style="text-align: right; padding-right: 20px;">RegDA</th>
                        <td style="text-align: left;">
                            It is developed to target human pose estimation and introduces an adversarial regressor 
                            based on the Kullback-Leibler divergence between domains to narrow their gap. [<a id="a19" onclick="scrl('r19')" class="hp">19</a>]
                        </td>
                    </tr>

                </tbody>
            </table>
        </div>
        

        <p class="fw-bold pt-5" style="text-align: left; font-size: 2rem;">Metrics</p>
        <p style="text-align: left;">
            Another aspect on which there is still a lot of confusion and open debate is about how systems should be 
            evaluated. There are multiple competing notions of fairness and ways to quantify it. Previous studies 
            measure group fairness by accuracy difference between advantaged and disadvantaged subgroups. However, 
            this goal has been criticized in philosophy and ethics literature. Purely minimizing the gap between 
            subgroup performance, may lead to choosing a model with worse accuracy for all subgroups, which is 
            Pareto inefficient and violates the ethical principles of beneficence and non-maleficence. Thus, we 
            analyze several existing group fairness criteria and highlight the lack of a metric that properly 
            aggregates overall performance and fairness level to assess the quality of a model.
        </p>
        <p class="pt-3 text-start d-flex" style="text-align: left;">The metrics used in our evaluation are:</p>
        <div class="table-responsive p-abs" style="border: 0;">
            <table class="table table-sm table-borderless">
                <tbody>
                    <tr>
                        <th style="text-align: right; border-bottom: 1px solid black;" colspan="2">Classification</th>
                    </tr>
                    <tr>
                        <th style="text-align: right; padding-right: 20px;">Acc.</th>
                        <td style="text-align: left;">Classification accuracy.</td>
                    </tr>
                    <tr>
                        <th style="text-align: right; padding-right: 20px;">MGA</th>
                        <td style="text-align: left;">
                            <i>Max Group Accuracy</i>, the classification accuracy of the best-performing protected group.
                        </td>
                    </tr>
                    <tr>
                        <th style="text-align: right; padding-right: 20px;">mGA</th>
                        <td style="text-align: left;">
                            <i>Min Group Accuracy</i>, the classification accuracy of the worst-performing protected group.
                        </td>
                    </tr>
                    <tr>
                        <th style="text-align: right; padding-right: 20px;">DA</th>
                        <td style="text-align: left;">
                            <i>Difference in Accuracy</i>. Measures Fairness by computing the difference in accuracy 
                            between protected groups.
                            \[DA = MGA - mGA\] 
                            The lower, the better.
                        </td>
                    </tr>
                    <tr>
                        <th style="text-align: right; padding-right: 20px;">DEO</th>
                        <td style="text-align: left;">
                            <i>Difference in Equal Opportunity</i>. Measures Fairness by 
                            \[|P(\hat{y}=1|y=1,a=0) - P(\hat{y}=1|y=1,a=1)|\]
                            where \(\hat{y}\) is the classifiers prediction, \(y\) is the target label and
                            \(a\) is the sensitive attribute. The lower, the better.
                        </td>
                    </tr>
                    <tr>
                        <th style="text-align: right; padding-right: 20px;">DEOdds</th>
                        <td style="text-align: left;">
                            <i>Difference in Equalized Odds</i> Measures Fairness by 
                            \[\sum_{t \in \{0,1\}}|P(\hat{y}=1|y=t,a=0) - P(\hat{y}=1|y=t,a=1)|\]
                            The lower, the better.
                        </td>
                    </tr>
                    <tr>
                        <th style="text-align: right; padding-right: 20px;">ΔDTO</th>
                        <td style="text-align: left;">
                            <i>Difference in Distance To Optimum</i>. A perfectly fair and accurate classifier exhibits
                            \([MGA^*, mGA^*] = [100, 100]\) (<i>Utopia</i>). Thus, we define as \(DTO\) 
                            (<i>Distance To Optimum</i>) as the L2 distance between \([MGA,mGA]\) of a method and the
                            <i>Utopia</i> point \([MGA^*, mGA^*]\). To estimate the relative performance of each method 
                            with respect to the baseline, we report 
                            \[\Delta DTO = DTO_{baseline} - DTO_{method}\]
                            The higher, the better.
                        </td>
                    </tr>

                    <tr>
                        <th style="text-align: right; border-bottom: 1px solid black;" colspan="2">Landmark Detection</th>
                    </tr>
                    <tr>
                        <td style="text-align: left;" colspan="2">
                            When dealing with landmark detection every data sample can be defined as 
                            \((\boldsymbol{x}, a, \boldsymbol{Y})\), where
                            \(\boldsymbol{Y} \in \mathbb{R}^{K \times 2}\) is a set of \(\boldsymbol{y}_{1,...,K}\) 
                            landmark coordinates. The reference metric for this task is the Normalized Mean Error
                            (<i>NME</i>) calculated as:
                            \[NME(\boldsymbol{Y}, \hat{\boldsymbol{Y}}) = \frac{1}{N} \sum_{i=1}^K \frac{\| \boldsymbol{y}_i - \hat{\boldsymbol{y}}_i \|_2}{D} \]
                            where \(D\) is a normalization factor, usually chosen as the interocular distance for face images. The lower
                            the <i>NME</i>, the better.
                        </td>
                    </tr>
                    <tr>
                        <th style="text-align: right; padding-right: 20px;">SDR</th>
                        <td style="text-align: left;">
                            <i>Success Detection Rate</i> is the percentage of images whose <i>NMEs</i> is less than a given threshold.
                            The higher, the better.
                        </td>
                    </tr>
                    <tr>
                        <th style="text-align: right; padding-right: 20px;">MGS</th>
                        <td style="text-align: left;">
                            <i>Max SDR</i>, the <i>SDR</i> of the best-performing protected group.
                        </td>
                    </tr>
                    <tr>
                        <th style="text-align: right; padding-right: 20px;">mGS</th>
                        <td style="text-align: left;">
                            <i>Min SDR</i>, the <i>SDR</i> of the worst-performing protected group.
                        </td>
                    </tr>
                    <tr>
                        <th style="text-align: right; padding-right: 20px;">DS</th>
                        <td style="text-align: left;">
                            <i>Difference in SDR</i> Measures Fairness by computing the difference in SDR 
                            between protected groups.
                            \[DS = MGS - mGS\] 
                            The lower, the better.
                        </td>
                    </tr>
                    <tr>
                        <th style="text-align: right; padding-right: 20px;">ΔDTO</th>
                        <td style="text-align: left;">
                            <i>Difference in Distance To Optimum</i>. The definition is analogous to the one given for the
                            classification task, but using <i>MGS</i> and <i>mGS</i>.
                        </td>
                    </tr>

                </tbody>
            </table>
        </div>

        <p class="fw-bold pt-5" style="text-align: left; font-size: 2rem;">Datasets</p>
        <div class="table-responsive p-abs" style="border: 0;">
            <table class="table table-sm table-borderless">
                <tbody>
                    <tr>
                        <th style="text-align: right; padding-right: 20px;">CelebFaces Attribute (CelebA)</th>
                        <td style="text-align: left;">
                            [<a id="a20" onclick="scrl('r20')" class="hp">20</a>] comprises 202,599
                            RGB face images of celebrities, each with 40 binary attribute annotations. 
                            We focus on the same subset of 13
                            reliable target attributes considered in 
                            [<a onclick="scrl('r1')" class="hp">1</a>, <a onclick="scrl('r17')" class="hp">17</a>],
                            as they can be labeled objectively, without being ambiguous for a human. 
                            We select <i>male</i> and <i>young</i> as protected attributes, and adopt the same
                            setting of [<a onclick="scrl('r17')" class="hp">17</a>], based on the official train/val/test splits.
                        </td>
                    </tr>
                    <tr>
                        <th style="text-align: right; padding-right: 20px;">COVID-19 Chest X-Ray</th>
                        <td style="text-align: left;">
                            [<a id="a21" onclick="scrl('r21')" class="hp">21</a>] is composed of 719 images
                            of chest x-ray coming from different online sources showing scans of patients affected by pulmonary diseases. Each
                            image has a structured label describing many attributes of
                            the patient. We focus on the <i>finding</i> attribute as target, considering the COVID-19 pathology, while <i>gender</i> is selected
                            as sensitive attribute. We split the dataset into 80/20% training/test sets, using 20% of the training split for validation.
                        </td>
                    </tr>
                    <tr>
                        <th style="text-align: right; padding-right: 20px;">Fitzpatrick17k</th>
                        <td style="text-align: left;">
                            [<a id="a22" onclick="scrl('r22')" class="hp">22</a>] is a collection of 16,577 clinical 
                            images depicting 114 skin conditions from two dermatology
                            atlases. The images are annotated with the six Fitzpatrick
                            skin type labels, that describe the skin phenotype’s sun reactivity. 
                            The dataset is widely used in algorithmic fairness research [<a id="a23" onclick="scrl('r23')" class="hp">23</a>]. 
                            We classify whether the dermatological
                            condition in each picture is either <i>benign/non-neoplastic</i> or
                            <i>malignant</i> and we use <i>skin tone</i> as the protected attribute,
                            keeping only the examples belonging to <i>skin type I</i> (light)
                            and <i>skin type VI</i> (dark) of the Fitzpatrick scale. We split
                            the dataset into 80/20% training/test sets, using 20% of the
                            training split for validation
                        </td>
                    </tr>
                    <tr>
                        <th style="text-align: right; padding-right: 20px;">UTKFace</th>
                        <td style="text-align: left;">
                            [<a id="a24" onclick="scrl('r24')" class="hp">24</a>] consists of over 20k RGB face images 
                            characterized by great variability in terms of pose, facial expression, illumination, <i>etc.</i>, 
                            and present age, gender, and race
                            annotations. We focus on landmark localization (68 points)
                            considering the values <i>white</i> and <i>black</i> of the label <i>race</i> as
                            protected groups for the experiments related to <i>skin tone</i>.
                            Moreover, we define the <i>young</i> and <i>old</i> groups by collecting
                            respectively samples with the value of label <i>age</i> in 0-10 and
                            40-50 years old. Training/test division is in the proportion
                            80/20% with 20% of the training split used for validation.
                        </td>
                    </tr>
                </tbody>
            </table>
        </div>

        <p class="fw-bold pt-5" style="text-align: left; font-size: 2rem;">Results</p>
        <p style="text-align: left;">
            Here we present the main results of our experiments. Our PyTorch implementation covers
            all the methods evaluated in the benchmark to guarantee maximal transparency and reproducibility. 
            It can also easily include other methods for future benchmark extensions.
            Unless stated otherwise, for all the experiments we adopted the same validation protocol 
            described in [<a onclick="scrl('r17')" class="hp">17</a>].
        </p>
        
        <p class="fw-bold fs-6 pt-5" style="text-align: right; border-bottom: 1px solid black;">01. Classification Results</p>
        <div class="container results-tables">
            <p style="text-align: left;"><b>CelebA - 13 Attributes</b> <i>(gender)</i></p>
            <div class="table-responsive d-flex justify-content-center" id="celeba-13-attribs"></div>

            <p class="pt-5" style="text-align: left;"><b>CelebA - EyeBags</b> <i>(gender)</i></p>
            <div class="table-responsive d-flex justify-content-center" id="celeba-eyebags"></div>

            <p class="pt-5" style="text-align: left;"><b>CelebA - Chubby</b> <i>(gender)</i></p>
            <div class="table-responsive d-flex justify-content-center" id="celeba-chubby"></div>

            <p class="pt-5" style="text-align: left;"><b>COVID-19 Chest X-Ray</b> <i>(gender)</i></p>
            <div class="table-responsive d-flex justify-content-center" id="covid-cxr"></div>

            <p class="pt-5" style="text-align: left;"><b>Fitzpatrick17k</b> <i>(skin tone)</i></p>
            <div class="table-responsive d-flex justify-content-center" id="fitzpatrick"></div>
        </div>
        
        <p class="fw-bold fs-6 pt-5" style="text-align: right; border-bottom: 1px solid black;">02. Landmark Detection Results</p>
        <div class="container results-tables">
            <p class="fw-bold" style="text-align: left;">UTKFace - Landmark Detection @ 8% NME</p>
            <div class="table-responsive d-flex justify-content-center" id="utkface-skintone"></div>
            <div class="table-responsive d-flex justify-content-center" id="utkface-age"></div>
        </div>

        <p class="fw-bold fs-6 pt-5" style="text-align: right; border-bottom: 1px solid black;">03. Model Transferability</p>
        <div class="container results-tables">
            <p class="fw-bold" style="text-align: left;">CelebA - EyeBags</p>
            <div class="table-responsive d-flex justify-content-center" id="celeba-eyebags-transfer"></div>

            <p class="fw-bold pt-5" style="text-align: left;">UTKFace - Landmark Detection @ 8% NME</p>
            <div class="table-responsive d-flex justify-content-center" id="utkface-transfer"></div>
        </div>
        
        <hr style="border-bottom: 1px black; width: 100%;"/>

        <p class="fw-bold pt-2" style="text-align: left; font-size: 1rem;">References</p>
        <div class="table-responsive p-abs" style="border: 0;">
            <table class="table table-sm table-borderless" style="font-size: 0.85rem;">
                <tbody id="ref-body"></tbody>
            </table>
        </div>

        <hr style="border-bottom: 1px black; width: 100%;"/>
        
        <!--h3 class="pt-5"><b>xxxx</b></h3>

        <p class="pt-3"><b>xxxx<sup>1</sup>, xxxx<sup>1</sup>, xxxx<sup>3</sup>,
                xxxx<sup>1,2</sup></b></p>
        <div class="container d-flex justify-content-evenly">
            <p><sup>1</sup>xxxx</p>
            <p><sup>2</sup>xxxx</p>
            <p><sup>3</sup>xxxx</p>
        </div>

        <div class="container d-flex justify-content-center pt-3 pb-3">
            <a href="xxxx" target="_blank"><button type="button" class="btn btn-dark btn-small m-2"><i
                        class="fa fa-newspaper-o"></i> Paper</button></a>
            <a href="xxxx" target="_blank"><button type="button" class="btn btn-dark btn-small m-2"><i
                        class="ai ai-arxiv"></i> ArXiv</button></a>
            <a href="xxxx" target="_blank"><button type="button"
                    class="btn btn-dark btn-small m-2"><i class="fa fa-github"></i> Code</button></a>
        </div>

        <img src="xxxx" class="img-fluid pt-5 pb-3 img-teaser" alt="teaser">


        <p class="text-start" style="text-align: justify;"><b>Overview:</b> xxxx</p>

        <h5 class="fw-bold pt-5">Abstract</h5>
        <p class="fst-italic p-abs" style="text-align: justify;">
                xxxx
        </p-->

    </div>
</body>

</html>